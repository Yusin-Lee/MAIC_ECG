{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Custom_dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_train = pd.read_csv('./dataset/ECG_adult_age_train.csv')\n",
    "child_train = pd.read_csv('./dataset/ECG_child_age_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_file_path = './dataset/ECG_adult_numpy_train/'\n",
    "child_file_path = './dataset/ECG_child_numpy_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_train_data, adult_valid_data = train_test_split(adult_train,shuffle=True, random_state=42, test_size=0.1)\n",
    "adult_train_data.reset_index(inplace=True,drop=True)\n",
    "adult_valid_data.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_train_dataset = CustomDataset(adult_file_path,adult_train_data)\n",
    "adult_valid_dataset = CustomDataset(adult_file_path,adult_valid_data)\n",
    "child_dataset = CustomDataset(child_file_path,child_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adult_loader = torch.utils.data.DataLoader(adult_train_dataset, batch_size = 64)\n",
    "valid_adult_loader = torch.utils.data.DataLoader(adult_valid_dataset, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\colon\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Temporalblock(torch.nn.Module):\n",
    "    def __init__(self, N, K, MP_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = torch.nn.LazyConv1d(N, K, stride=1, padding = 'same')\n",
    "        self.batchnorm_layers = torch.nn.BatchNorm1d(N)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=MP_factor, stride=MP_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.batchnorm_layers(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "class FCblock(torch.nn.Module):\n",
    "    def __init__(self, N, dropout):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.LazyLinear(N)\n",
    "        self.batchnorm_layers = torch.nn.BatchNorm1d(N)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.batchnorm_layers(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class spatial_block(torch.nn.Module):\n",
    "    def __init__(self,N, K, MP_factor):\n",
    "        super().__init__()\n",
    "        self.conv_layers = torch.nn.LazyConv2d(N, (K,1),padding='same')\n",
    "        self.batchnorm_layer = torch.nn.BatchNorm2d(N)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.maxpool = torch.nn.MaxPool1d(kernel_size=MP_factor, stride=MP_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.batchnorm_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Custom_model(torch.nn.Module):\n",
    "    def __init__(self,N, K, MP_factor):\n",
    "        super().__init__()\n",
    "        self.first_block = Temporalblock(16, 7, 2)\n",
    "        self.temporal_block = self._make_layer(N, K, MP_factor)\n",
    "        self.spatial_block = spatial_block(128, 12, 2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = FCblock(128, 0.2)\n",
    "        self.fc2 = FCblock(64, 0.2)\n",
    "\n",
    "        self.linear_out = torch.nn.Linear(64,1)\n",
    "    def _make_layer(self, N, K, MP_factor):\n",
    "        layer = []\n",
    "        for n, k, mp in zip(N, K, MP_factor):\n",
    "            layer.append(Temporalblock(n, k, mp))\n",
    "        return torch.nn.Sequential(*layer)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.first_block(x)\n",
    "        x = self.temporal_block(x)\n",
    "        x = self.spatial_block(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.linear_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "N = (16,32,32,64,64,64,64)\n",
    "K = (5,5,5,5,3,3,3)\n",
    "MP_factor = (4,2,4,2,2,2,2)\n",
    "model = Custom_model(N, K, MP_factor)\n",
    "model.to('cuda')\n",
    "print('model_ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Custom_model(\n",
       "  (first_block): Temporalblock(\n",
       "    (conv_layers): LazyConv1d(0, 16, kernel_size=(7,), stride=(1,), padding=same)\n",
       "    (batchnorm_layers): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (temporal_block): Sequential(\n",
       "    (0): Temporalblock(\n",
       "      (conv_layers): LazyConv1d(0, 16, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      (batchnorm_layers): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Temporalblock(\n",
       "      (conv_layers): LazyConv1d(0, 32, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      (batchnorm_layers): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Temporalblock(\n",
       "      (conv_layers): LazyConv1d(0, 32, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      (batchnorm_layers): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): Temporalblock(\n",
       "      (conv_layers): LazyConv1d(0, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      (batchnorm_layers): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): Temporalblock(\n",
       "      (conv_layers): LazyConv1d(0, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
       "      (batchnorm_layers): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (5): Temporalblock(\n",
       "      (conv_layers): LazyConv1d(0, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
       "      (batchnorm_layers): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (6): Temporalblock(\n",
       "      (conv_layers): LazyConv1d(0, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
       "      (batchnorm_layers): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (spatial_block): spatial_block(\n",
       "    (conv_layers): LazyConv2d(0, 128, kernel_size=(12, 1), stride=(1, 1), padding=same)\n",
       "    (batchnorm_layer): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): FCblock(\n",
       "    (linear): LazyLinear(in_features=0, out_features=128, bias=True)\n",
       "    (batchnorm_layers): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc2): FCblock(\n",
       "    (linear): LazyLinear(in_features=0, out_features=64, bias=True)\n",
       "    (batchnorm_layers): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (linear_out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),lr = 3e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', min_lr = 1e-5, patience=2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max= 20, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/491 [00:00<?, ?it/s]C:\\Users\\SNUH벤처\\AppData\\Local\\Temp\\ipykernel_9864\\2323642984.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(batch[2],dtype=torch.float32).to('cuda')\n",
      "C:\\Users\\SNUH벤처\\AppData\\Local\\Temp\\ipykernel_9864\\2323642984.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output = model(torch.tensor(inputs, dtype = torch.float32))\n",
      "c:\\Anaconda\\envs\\colon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\Convolution.cpp:1004.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "  0%|          | 0/491 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\study\\MAIC_ECG\\Baseline.ipynb 셀 10\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m inputs \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m5000\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(batch[\u001b[39m2\u001b[39m],dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39;49mtensor(inputs, dtype \u001b[39m=\u001b[39;49m torch\u001b[39m.\u001b[39;49mfloat32))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\colon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\study\\MAIC_ECG\\Baseline.ipynb 셀 10\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_block(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemporal_block(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspatial_block(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\colon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\study\\MAIC_ECG\\Baseline.ipynb 셀 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layers(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatchnorm_layer(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/study/MAIC_ECG/Baseline.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\colon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\colon\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\colon\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:410\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexpected 4D input (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n",
      "\u001b[1;31mValueError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "for epochs in range(20):\n",
    "    print('train_run')\n",
    "    for batch in tqdm(train_adult_loader):\n",
    "        inputs = batch[0].view(-1,12,5000).to('cuda')\n",
    "        label = torch.tensor(batch[2],dtype=torch.float32).to('cuda')\n",
    "        output = model(torch.tensor(inputs, dtype = torch.float32))\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    valid_loss = 0\n",
    "    print('valid_run')\n",
    "    for batch in tqdm(valid_adult_loader):\n",
    "        inputs = batch[0].view(-1,12,5000).to('cuda')\n",
    "        label = torch.tensor(batch[2],dtype=torch.float32).to('cuda')\n",
    "        with torch.no_grad():\n",
    "            output = model(torch.tensor(inputs, dtype = torch.float32))\n",
    "        loss = criterion(output, label)\n",
    "        valid_loss += loss\n",
    "    loss_check = valid_loss/len(valid_adult_loader)\n",
    "    print(loss_check)\n",
    "    scheduler.step(loss_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55 [00:00<?, ?it/s]C:\\Users\\SNUH벤처\\AppData\\Local\\Temp\\ipykernel_1180\\1463697845.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(batch[2],dtype=torch.float32).to('cuda')\n",
      "C:\\Users\\SNUH벤처\\AppData\\Local\\Temp\\ipykernel_1180\\1463697845.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output = model(torch.tensor(inputs, dtype = torch.float32))\n",
      "100%|██████████| 55/55 [00:02<00:00, 24.48it/s]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "labels = []\n",
    "for batch in tqdm(valid_adult_loader):\n",
    "    inputs = batch[0].view(-1,12,5000).to('cuda')\n",
    "    label = torch.tensor(batch[2],dtype=torch.float32).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        output = model(torch.tensor(inputs, dtype = torch.float32))\n",
    "    pred += output.detach().cpu().tolist()\n",
    "    labels += label.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.689496059193717"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(pred,labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
